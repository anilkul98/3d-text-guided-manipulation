{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3136e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop\n",
    "import numpy as np\n",
    "import os\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "from model import _G\n",
    "from util.visualization import visualize_occupancy\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import clip\n",
    "import cv2\n",
    "\n",
    "\n",
    "with open('config.yaml') as fp:\n",
    "    config = yaml.load(fp, Loader=SafeLoader)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"using cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c98e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(img):\n",
    "    resized_img = img.permute(0, 2, 3, 1).detach().cpu().numpy()\n",
    "\n",
    "    plt.imshow(resized_img[0])\n",
    "    plt.show()\n",
    "    \n",
    "def generate_model(Z):\n",
    "    with torch.no_grad():\n",
    "        generation = G(Z)\n",
    "        generation_cpu = (generation.detach().cpu().numpy()>0.5).astype(int)\n",
    "        visualize_occupancy(generation_cpu[0].squeeze())        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c71fd49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Armut\\anaconda3\\envs\\mlfor3d\\lib\\site-packages\\torchvision\\transforms\\transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_G(\n",
       "  (layer1): Sequential(\n",
       "    (0): ConvTranspose3d(256, 256, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): ConvTranspose3d(256, 128, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): ConvTranspose3d(128, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): ConvTranspose3d(64, 32, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer5): Sequential(\n",
       "    (0): ConvTranspose3d(32, 1, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "mean = torch.as_tensor((0.48145466, 0.4578275, 0.40821073), dtype=torch.float, device=device)\n",
    "std = torch.as_tensor((0.26862954, 0.26130258, 0.27577711), dtype=torch.float, device=device)\n",
    "if mean.ndim == 1:\n",
    "    mean = mean.view(-1, 1, 1)\n",
    "if std.ndim == 1:\n",
    "    std = std.view(-1, 1, 1)\n",
    "\n",
    "transf = Compose([Resize(224, interpolation=Image.BICUBIC), CenterCrop(224)])\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "G_path = os.path.join(\"checkpoints\", \"G_300.pth\")\n",
    "\n",
    "G = _G(config[\"dim\"], config[\"latent_len\"])\n",
    "\n",
    "G.load_state_dict(torch.load(G_path))\n",
    "G.to(device)\n",
    "G.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb78cc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nof_samples = 1\n",
    "Z = torch.Tensor(nof_samples, config[\"latent_len\"]).normal_(0, 0.33).to(device)\n",
    "generation = G(Z)\n",
    "generation_cpu = (generation.detach().cpu().numpy()>0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d99861b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb8ae7d8c57468ab7347b09198dfd60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_occupancy(generation_cpu[0].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0fe4f82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2f0d35fc4645fab7f0513dffae835b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nof_samples = 1\n",
    "Z1 = torch.Tensor(nof_samples, config[\"latent_len\"]).normal_(0, 0.33).to(device)\n",
    "generate_model(Z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "68e51be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f78ec405a63495f91b852a9ea1b59c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nof_samples = 1\n",
    "Z2 = torch.Tensor(nof_samples, config[\"latent_len\"]).normal_(0, 0.33).to(device)\n",
    "generate_model(Z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cd4a3674",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e12c8ad86147eb8d26d78c021f50eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nof_samples = 1\n",
    "Z3 = torch.Tensor(nof_samples, config[\"latent_len\"]).normal_(0, 0.33).to(device)\n",
    "generate_model(Z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "02618a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2742250aa6ae4b67aae879a53d20ea5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_model(Z3+Z2-Z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962c8e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846c491d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "941e0e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af183eb633b47b79addda5606d89f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nof_samples = 1\n",
    "Z = torch.Tensor(nof_samples, config[\"latent_len\"]).normal_(0, 0.33).to(device)\n",
    "generate_model(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c930857e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe95a21b5ab341a3bcb0e281954bf569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Similarity: 0.68408203125\n",
      "Iteration: 1 Similarity: 0.6845703125\n",
      "Iteration: 2 Similarity: 0.6845703125\n",
      "Iteration: 3 Similarity: 0.6845703125\n",
      "Iteration: 4 Similarity: 0.6845703125\n",
      "Iteration: 5 Similarity: 0.68408203125\n",
      "Iteration: 6 Similarity: 0.6845703125\n",
      "Iteration: 7 Similarity: 0.6845703125\n",
      "Iteration: 8 Similarity: 0.68505859375\n",
      "Iteration: 9 Similarity: 0.6845703125\n",
      "Iteration: 10 Similarity: 0.6845703125\n",
      "Iteration: 11 Similarity: 0.6845703125\n",
      "Iteration: 12 Similarity: 0.6845703125\n",
      "Iteration: 13 Similarity: 0.68505859375\n",
      "Iteration: 14 Similarity: 0.68505859375\n",
      "Iteration: 15 Similarity: 0.6845703125\n",
      "Iteration: 16 Similarity: 0.68505859375\n",
      "Iteration: 17 Similarity: 0.68505859375\n",
      "Iteration: 18 Similarity: 0.68505859375\n",
      "Iteration: 19 Similarity: 0.68505859375\n",
      "Iteration: 20 Similarity: 0.68505859375\n",
      "Iteration: 21 Similarity: 0.6845703125\n",
      "Iteration: 22 Similarity: 0.68505859375\n",
      "Iteration: 23 Similarity: 0.68505859375\n",
      "Iteration: 24 Similarity: 0.685546875\n",
      "Iteration: 25 Similarity: 0.6845703125\n",
      "Iteration: 26 Similarity: 0.68505859375\n",
      "Iteration: 27 Similarity: 0.685546875\n",
      "Iteration: 28 Similarity: 0.68505859375\n",
      "Iteration: 29 Similarity: 0.68505859375\n",
      "Iteration: 30 Similarity: 0.68505859375\n",
      "Iteration: 31 Similarity: 0.685546875\n",
      "Iteration: 32 Similarity: 0.68505859375\n",
      "Iteration: 33 Similarity: 0.68505859375\n",
      "Iteration: 34 Similarity: 0.68505859375\n",
      "Iteration: 35 Similarity: 0.68505859375\n",
      "Iteration: 36 Similarity: 0.685546875\n",
      "Iteration: 37 Similarity: 0.68505859375\n",
      "Iteration: 38 Similarity: 0.68505859375\n",
      "Iteration: 39 Similarity: 0.685546875\n",
      "Iteration: 40 Similarity: 0.68505859375\n",
      "Iteration: 41 Similarity: 0.68505859375\n",
      "Iteration: 42 Similarity: 0.685546875\n",
      "Iteration: 43 Similarity: 0.685546875\n",
      "Iteration: 44 Similarity: 0.68505859375\n",
      "Iteration: 45 Similarity: 0.685546875\n",
      "Iteration: 46 Similarity: 0.685546875\n",
      "Iteration: 47 Similarity: 0.685546875\n",
      "Iteration: 48 Similarity: 0.68505859375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2eaa360daba429e80909b3afbe0673d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 49 Similarity: 0.68505859375\n"
     ]
    }
   ],
   "source": [
    "nof_iterations = 50\n",
    "\n",
    "text = clip.tokenize([\"airliner\"]).to(device)\n",
    "text_features = model.encode_text(text)\n",
    "\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "nof_samples = 1\n",
    "#Z = torch.Tensor(nof_samples, config[\"latent_len\"]).normal_(0, 0.33).to(device)\n",
    "\n",
    "Z.requires_grad_()\n",
    "\n",
    "optimizer = torch.optim.Adam([Z], lr=0.001)\n",
    "\n",
    "for i in range(nof_iterations):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    generation = G(Z)\n",
    "    \n",
    "    if i==0 or i==(nof_iterations-1):\n",
    "        generation_cpu = (generation.detach().cpu().numpy()>0.5).astype(int)\n",
    "        visualize_occupancy(generation_cpu[0].squeeze())\n",
    "\n",
    "    image_side_one = generation.amax(dim=2)\n",
    "    image_side_two = generation.amax(dim=3)\n",
    "    image_side_three = generation.amax(dim=4)\n",
    "    \n",
    "    image_sides = torch.concat([image_side_one, image_side_two, image_side_three])\n",
    "\n",
    "    broadcasted_img = torch.broadcast_to(image_sides, [3, 3, 32, 32])\n",
    "\n",
    "    img = (transf(broadcasted_img)).sub_(mean).div_(std)\n",
    "\n",
    "    image_features = model.encode_image(img)\n",
    "\n",
    "    print(\"Iteration:\", i, \"Similarity:\", torch.nn.functional.cosine_similarity(image_features, (text_features[0]).unsqueeze(0)).sum().item())\n",
    "    \n",
    "    cos_sim = -1*torch.nn.functional.cosine_similarity(image_features, (text_features[0]).unsqueeze(0))\n",
    "    \n",
    "    cos_sim.sum().backward(retain_graph=True)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    img_print = torch.concat([img[0], img[1], img[2]], axis=2).unsqueeze(dim=0).permute(0, 2, 3, 1).detach().cpu().numpy()[0]\n",
    "    img_print = (img_print-img_print.min())/(img_print.max()-img_print.min())\n",
    "    \n",
    "    plt.imsave(os.path.join(\"images\", f\"{i+1:02d}.jpg\"), img_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6fb47dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = 'images'\n",
    "video_name = 'video.mp4'\n",
    "\n",
    "images = [img for img in os.listdir(image_folder) if img.endswith(\".jpg\")]\n",
    "frame = cv2.imread(os.path.join(image_folder, images[0]))\n",
    "height, width, layers = frame.shape\n",
    "\n",
    "video = cv2.VideoWriter(video_name, 0, 4, (width,height))\n",
    "\n",
    "for image in images:\n",
    "    video.write(cv2.imread(os.path.join(image_folder, image)))\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61108851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2e4e58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c971a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b11ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08addb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8658688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
